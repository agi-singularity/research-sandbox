{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dd571d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import fetch_20newsgroups, load_files\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import TextVectorization, Embedding, Dense, Lambda\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba0e1c6",
   "metadata": {},
   "source": [
    "# Prepare the Data\n",
    "\n",
    "Will use the following datasets for training:\n",
    "1. 20 Newsgroups: Roughly 20k news articles, no ads.  This can be obtained from sklearn.datasets\n",
    "2. Kaggle Spam Filter dataset: 5.7k emails with a mix of spam and ham.  https://www.kaggle.com/karthickveerakumar/spam-filter\n",
    "3. Kaggle Text Classified Ads dataset: 97k job and real estate advertisements (only real estate ads are used, as job ads were found to adversely impact model results).  https://www.kaggle.com/overflow012/playing-with-ads\n",
    "4. Kaggle All the News dataset: 150k news articles.  https://www.kaggle.com/snapcrack/all-the-news\n",
    "5. Kaggle Enron Emails Spam dataset: Enron emails separated into spam/ham, ~17k spam.  https://www.kaggle.com/wanderfj/enron-spam\n",
    "\n",
    "Normally ads are a small portion of all documents.  These datasets will be combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e27a5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(row):\n",
    "    return row.decode('unicode_escape').encode('ascii', 'ignore')\n",
    "\n",
    "\n",
    "def load_enron_spam_emails():\n",
    "    # load emails from text files and filter to spam only\n",
    "    x, y = [], []\n",
    "    for i in range(1,7):\n",
    "        emails = load_files(f\"enron_spam/enron{i}\")\n",
    "        x = np.append(x, emails.data)\n",
    "        y = np.append(y, emails.target)\n",
    "    df = pd.DataFrame({\"text\": x[np.where(y==1)], \"ad\": 1})\n",
    "    df['text'] = df.text.apply(clean_text)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_classified_ads():\n",
    "    # load text classified ads data, filters to real estate ads only, removes duplicates\n",
    "    df = pd.read_csv('kaggle_text_classified_ads.csv')\n",
    "    df = df[df['catid'] != 2].drop_duplicates()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d97260eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-21a317b5d330>:2: DeprecationWarning: invalid escape sequence '\\ '\n",
      "  return row.decode('unicode_escape').encode('ascii', 'ignore')\n",
      "<ipython-input-2-21a317b5d330>:2: DeprecationWarning: invalid escape sequence '\\\r",
      "'\n",
      "  return row.decode('unicode_escape').encode('ascii', 'ignore')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>ad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SEOUL, South Korea  —   President Trump assure...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Swedish man is facing charges in court after...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(CNN) Donald Trump has made one thing clear, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'Subject: microcap stock report\\r\\ngrant ,\\r\\...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'Subject: 30 - vl . . . benz annal\\r\\nhtml\\r\\...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  ad\n",
       "0  SEOUL, South Korea  —   President Trump assure...   0\n",
       "1  A Swedish man is facing charges in court after...   0\n",
       "2   (CNN) Donald Trump has made one thing clear, ...   0\n",
       "3  b'Subject: microcap stock report\\r\\ngrant ,\\r\\...   1\n",
       "4  b'Subject: 30 - vl . . . benz annal\\r\\nhtml\\r\\...   1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get datasets: newgroups = all news, ads = all ads, emails = mixed\n",
    "data_newsgroups = fetch_20newsgroups(subset='all', shuffle=True, remove=('headers', 'footers', 'quotes'))\n",
    "data_emails = pd.read_csv('kaggle_spam_filter_data.csv')\n",
    "data_ads = load_classified_ads()\n",
    "data_news = pd.concat([\n",
    "    pd.read_csv('kaggle_all_the_news_1.csv', index_col=0), \n",
    "    # pd.read_csv('kaggle_all_the_news_2.csv', index_col=0)\n",
    "])\n",
    "data_enron = load_enron_spam_emails()\n",
    "\n",
    "# combine datasets\n",
    "data_all = pd.concat([\n",
    "    data_emails.rename(columns={\"spam\": \"ad\"}),\n",
    "    pd.DataFrame({\"text\": data_ads['value'], \"ad\": 1}),\n",
    "    pd.DataFrame({\"text\": data_newsgroups.data, \"ad\": 0}),\n",
    "    pd.DataFrame({\"text\": data_news.content, \"ad\": 0}),\n",
    "    data_enron,\n",
    "])\n",
    "assert(len(data_newsgroups.data) + len(data_emails) + len(data_ads) + len(data_news) + len(data_enron) == len(data_all))\n",
    "data_all = data_all.sample(frac=1.0).reset_index(drop=True)  # shuffle\n",
    "data_all.to_csv(\"training_data.csv\")\n",
    "data_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21116c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Composition:\n",
      " 18846 news articles, all labeled ham\n",
      " 5728 emails, split between spam and ham\n",
      " 2596 classified ads, all labeled spam\n",
      " 50000 news articles, all labeled ham\n",
      " 17170 emails, all labeled spam\n",
      " Total Size: 94340\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Training Dataset Composition:\\n\",\n",
    "    len(data_newsgroups.data), \"news articles, all labeled ham\\n\", \n",
    "    len(data_emails), \"emails, split between spam and ham\\n\",\n",
    "    len(data_ads), \"classified ads, all labeled spam\\n\",\n",
    "    len(data_news), \"news articles, all labeled ham\\n\",\n",
    "    len(data_enron), \"emails, all labeled spam\\n\",\n",
    "    \"Total Size:\", len(data_all)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "486e6a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    73206\n",
       "1    21134\n",
       "Name: ad, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all.ad.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a8ca997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 75472 Valid: 9434 Test: 9434\n"
     ]
    }
   ],
   "source": [
    "# data has already been shuffled\n",
    "\n",
    "# split into train, val, test sets\n",
    "test_split = 0.1\n",
    "nbr_val_test_samples = int(test_split * 2 * len(data_all))\n",
    "nbr_test_samples = int(test_split * len(data_all))\n",
    "train = data_all.iloc[:-nbr_val_test_samples]\n",
    "valid = data_all.iloc[-nbr_val_test_samples:-nbr_test_samples]\n",
    "test = data_all.iloc[-nbr_test_samples:]\n",
    "assert(len(train) + len(valid) + len(test) == len(data_all))\n",
    "print(\"Train:\", len(train), \"Valid:\", len(valid), \"Test:\", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d168ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize the text (give every token a number)\n",
    "max_sequence_length = 200\n",
    "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=max_sequence_length)\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(data_all.iloc[:-nbr_val_test_samples]['text'].values).batch(128)\n",
    "vectorizer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61344d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the vectorizer's config and weights\n",
    "pickle.dump(\n",
    "    {\n",
    "        'config': vectorizer.get_config(),\n",
    "         'weights': vectorizer.get_weights()\n",
    "    }, \n",
    "    open(\"tv_layer.pkl\", \"wb\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd3a7d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary map of word to index\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(vocab, range(len(vocab))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "255daa3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'the', 'to', 'of']\n",
      "[    2  6400  2480    11     2 15970]\n"
     ]
    }
   ],
   "source": [
    "# sense checks\n",
    "print(vectorizer.get_vocabulary()[:5])\n",
    "output = vectorizer([[\"the cat sat on the mat\"]])\n",
    "print(output.numpy()[0, :6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f193b5c4",
   "metadata": {},
   "source": [
    "# Load Pre-Trained Embeddings\n",
    "\n",
    "Will use GloV3 embeddings.  https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "There are different embeddings available.  This model will use the Wikipedia + Gigaword 6B dataset; a corpus of Wikipedia articles and Gigaword (newswire) articles with 6 billion tokens and 400k vocab size.  The dimensionality is 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1944f1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
    "# !unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29b246f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors of 300 dimensions.\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 300\n",
    "path_to_glove_file = f\"glove.6B.{embed_dim}d.txt\"\n",
    "\n",
    "# map all words in the embedding vocabulary to their numpy GloVe representation\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(f\"Found {len(embeddings_index)} word vectors of {embed_dim} dimensions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7092a523",
   "metadata": {},
   "source": [
    "# Embed Each Document\n",
    "\n",
    "The model will use GloVe embeddings.  A neural network will be built to:\n",
    "1. vectorize the text\n",
    "2. embed with GloVe embeddings\n",
    "3. aggregate up to the document level\n",
    "4. re-scale the embeddings with a sigmoid function\n",
    "\n",
    "Step 3 will sum the embeddings so that latent features that are heavily represented by the words in a document will have the largest values.  This will aggregate the embeddings from the word level to the document level.  Step 4 will re-scale the aggregated embeddings with a sigmoid function, which will push latent features with small values towards 0 and latent features with large values towards 1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bed4861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 17948 words and had 2052 out of vocabulary words\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(vocab) + 2  # vocab size + 2 for padding and OOV token (out of vocabulary)\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embed_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(f\"Converted {hits} words and had {misses} out of vocabulary words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50649292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20002, 300)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6336bd23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization (TextVec  (None, 200)              0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 200, 300)          6000600   \n",
      "                                                                 \n",
      " lambda (Lambda)             (200, 300)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,000,600\n",
      "Trainable params: 0\n",
      "Non-trainable params: 6,000,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create an embedding layer\n",
    "embedding_layer = Embedding(\n",
    "    input_dim=num_tokens,\n",
    "    output_dim=embed_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.e**(-1 * x))\n",
    "\n",
    "\n",
    "def sum_layer(x):\n",
    "    return sigmoid(tf.reduce_sum(input_tensor=x, axis=0, keepdims=False))\n",
    "\n",
    "\n",
    "# build a simple neural network that applies the embedding to the input text and returns an embedding per document\n",
    "# the network will take the document text strings as input, so doc length does not matter\n",
    "# the network will not be trained, it just needs to apply the transformation in a single forward pass\n",
    "string_input = keras.Input(shape=(1,), dtype=\"string\")\n",
    "vectorized_input = vectorizer(string_input)\n",
    "x = embedding_layer(vectorized_input)\n",
    "x = Lambda(sum_layer)(x)  # aggregate from sentence level to document level\n",
    "model = keras.Model(string_input, x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ada225db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: embedding_model/assets\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model.save('embedding_model')\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "318ff666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize the documents in preparation for a forward pass\n",
    "x_train = [np.array(doc) for doc in train.text]\n",
    "x_valid = [np.array(doc) for doc in valid.text]\n",
    "x_test = [np.array(doc) for doc in test.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91d1a8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass to embed every document\n",
    "x_train = [model(d) for d in x_train]\n",
    "x_valid = [model(d) for d in x_valid]\n",
    "x_test = [model(d) for d in x_test]\n",
    "\n",
    "with open('x_train.pkl', 'wb') as f:\n",
    "    pickle.dump(x_train, f)\n",
    "with open('x_valid.pkl', 'wb') as f:\n",
    "    pickle.dump(x_train, f)\n",
    "with open('x_test.pkl', 'wb') as f:\n",
    "    pickle.dump(x_train, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6166c7",
   "metadata": {},
   "source": [
    "# Train Model\n",
    "\n",
    "The XGBoost model will fit to the document level embeddings, which are 300 latent features that have been aggregated from the word content of the documents.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9585360",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train.ad\n",
    "y_valid = valid.ad\n",
    "y_test = test.ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26686543",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Input data can not be a list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-6fc40791b142>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0my_valid_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0my_test_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, output_margin, ntree_limit, validate_features, base_margin)\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \"\"\"\n\u001b[0;32m--> 880\u001b[0;31m         test_dmatrix = DMatrix(data, base_margin=base_margin,\n\u001b[0m\u001b[1;32m    881\u001b[0m                                missing=self.missing, nthread=self.n_jobs)\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mntree_limit\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input data can not be a list.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         data, feature_names, feature_types = _convert_dataframes(\n",
      "\u001b[0;31mTypeError\u001b[0m: Input data can not be a list."
     ]
    }
   ],
   "source": [
    "xg = XGBClassifier(\n",
    "    booster='gbtree', \n",
    "    eta=0.3, \n",
    "    gamma=0, \n",
    "    max_depth=6, \n",
    "    alpha=0, \n",
    "    verbosity=0\n",
    ")\n",
    "xg.fit(np.stack(x_train), y_train)\n",
    "\n",
    "model_file_path = \"xgb_ad_classifier.pkl\"\n",
    "pickle.dump(xg, open(model_file_path, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "823dd1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_pred = xg.predict(np.stack(x_valid))\n",
    "y_test_pred = xg.predict(np.stack(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cfbe18ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Scores:\n",
      " Accuracy: 0.986326 \n",
      " Precision: 0.973219 \n",
      " Recall: 0.965370 \n",
      " F1: 0.969278\n",
      "Test Set Scores:\n",
      " Accuracy: 0.984418 \n",
      " Precision: 0.968008 \n",
      " Recall: 0.961020 \n",
      " F1: 0.964501\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Validation Set Scores:\\n\", \n",
    "    f\"Accuracy: {accuracy_score(y_valid, y_valid_pred):2f}\", \"\\n\",\n",
    "    f\"Precision: {precision_score(y_valid, y_valid_pred):2f}\", \"\\n\",\n",
    "    f\"Recall: {recall_score(y_valid, y_valid_pred):2f}\", \"\\n\",\n",
    "    f\"F1: {f1_score(y_valid, y_valid_pred):2f}\"\n",
    ")\n",
    "print(\n",
    "    \"Test Set Scores:\\n\", \n",
    "    f\"Accuracy: {accuracy_score(y_test, y_test_pred):2f}\", \"\\n\",\n",
    "    f\"Precision: {precision_score(y_test, y_test_pred):2f}\", \"\\n\",\n",
    "    f\"Recall: {recall_score(y_test, y_test_pred):2f}\", \"\\n\",\n",
    "    f\"F1: {f1_score(y_test, y_test_pred):2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de77b4e",
   "metadata": {},
   "source": [
    "# Test on Blackwing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "17792d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "blackwing_data = pd.read_csv('blackwing_3m_9k.csv')\n",
    "blackwing_data = blackwing_data[blackwing_data['disposition'].isin(['SELECT', 'IGNORE'])]\n",
    "blackwing_data = blackwing_data[['text', 'disposition']].drop_duplicates()\n",
    "# note that ignore DOES NOT mean it is an ad, but it is an indication that it might be\n",
    "blackwing_y = blackwing_data.disposition.map({\"SELECT\": 0, \"IGNORE\": 1})\n",
    "\n",
    "# vectorize the documents\n",
    "blackwing_test = [np.array(doc) for doc in blackwing_data.text]\n",
    "# embed the documents\n",
    "blackwing_test = [model(d) for d in blackwing_test]\n",
    "# make predictions\n",
    "blackwing_pred = xg.predict(blackwing_test)\n",
    "blackwing_pred_prob = xg.predict(blackwing_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "9dfe6b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blackwing Set Scores:\n",
      " Accuracy: 0.722973 \n",
      " Precision: 0.617647 \n",
      " Recall: 0.160305 \n",
      " F1: 0.254545\n"
     ]
    }
   ],
   "source": [
    "# score based on somewhat faulty ground truth\n",
    "print(\n",
    "    \"Blackwing Set Scores:\\n\", \n",
    "    f\"Accuracy: {accuracy_score(blackwing_y, blackwing_pred):2f}\", \"\\n\",\n",
    "    f\"Precision: {precision_score(blackwing_y, blackwing_pred):2f}\", \"\\n\",\n",
    "    f\"Recall: {recall_score(blackwing_y, blackwing_pred):2f}\", \"\\n\",\n",
    "    f\"F1: {f1_score(blackwing_y, blackwing_pred):2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "f3d36cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "blackwing_data['model_pred'] = blackwing_pred\n",
    "blackwing_data['model_pred_prob'] = blackwing_pred_prob\n",
    "blackwing_data.loc[blackwing_data.model_pred == 1, ['text', 'disposition', 'model_pred', 'model_pred_prob']].to_csv(\"blackwing_predicted_ads.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf47de5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
